# 웹 로봇

웹 로봇의 몇 가지 예
* 주식시장 서버에 매 분 HTTP GET 요청을 보내고, 여기서 얻은 데이터를 활용해 주가 추이 그래프를 생성하는 주식 그래프 로봇
* 월드 와이드 웹의 규모와 진화에 대한 통계 정보를 수집하는 웹 통계 조사 로봇. 웹을 떠돌면서 페이지의 개수를 세고, 각 페이지의 크기, 언어, 미디어 타입을 기록한다.
* 검색 데이터베이스를 만들기 위해 발견한 모든 문서를 수집하는 검색엔진 로봇
* 상품에 대한 가격 데이터베이스를 만들기 위해 온라인 쇼핑몰의 카탈로그에서 웹피이지를 수집하는 가격 비교 로봇

## 크롤러와 크롤링

웹 크롤러는 웹 페이지를 한 개 가져오고, 그 다음 그 페이지가 가리키는 모든 웹페이지를 가져오며 재ㅔ귀적으로 반복하는 방식으로 웹을 순회하는 로봇이다. 웹 링크를 재귀적으로 따라가는 로봇을 **크롤러** 또는 **스파이더**라고 부른다.

### 어디에서 시작하는가: '루트 집합'

크롤러가 방문을 시작하는 URL들의 초기 집합을 **루트 집합**(root set)이라고 부른다. 루트 집합을 고를 때, 모든 링크를 크롤링하면 결과적으로 관심 있는 웹페이지들의 대부분을 가져오게 될 수 있도록 충분히 다른 장소에서 URL들을 선택해야 한다.

일반적으로 좋은 루트 집합은 크고 인기 있는 웹 사이트, 새로 생성된 페이지들의 목록, 그리고 자주 링크되지 않는 잘 알려져 있지 않은 페이지들의 목록으로 구성되어 있다.

### 링크 추출과 상대 링크 정상화

크롤러는 웹을 돌아다니면서 꾸준히 HTML 문서를 검색한다. 크롤러는 검색한 각 페이지 안에 들어있는 URL 링크들을 파싱해서 크롤링할 페이지들의 목록에 추가해야 한다. 링크들을 추출하고 상대 링크를 절대 링크로 변환할 필요가 있다.

### 순환 피하기

로봇이 웹을 크롤링할 때, 루프나 순환에 빠지지 않도록 매우 조심해야 한다. 로봇들은 순환을 피하기 위해 반드시 그들이 어디를 방문했는지 알아야 한다. 순환은 로봇을 함정에 빠뜨려서 멈추게 하거나 진행을 느려지게 한다.

### 루프와 중복

* 순환은 크롤러를 루프에 빠뜨려서 꼼짝 못하게 만들 수 있다.
* 크롤러가 같은 페이지를 반복해서 가져오면 웹 서버의 부담이 된다. 실제 사용자도 사이트에 접근할 수 없도록 막아버리게 될 수도 있으며, 법적인 문제제기의 근거가 될 수도 있다.
* 루프 자체가 문제가 되지 않더라도, 크롤러는 많은 수의 중복된 페이지들을 가져오게 된다.

### 빵 부스러기의 흔적

방문한 곳을 지속적으로 추적하는 것은 쉽지 않다. 어떤 URL을 방문했는지 빠르게 판단하기 위해서는 복잡한 자료 구조를 사용할 필요가 있다. 이 자료 구조는 속도와 메모리 사용 면에서 효과적이어야 한다.

수억 개의 URL은 빠른 검색 구조를 요구하기 때문에 빠른 속도는 중요하다. 로봇은 어떤 URL이 방문했던 곳인지 빠르게 결정하기 위해 적어도 검색 트리나 해시 테이블을 필요로 할 것이다.

대규모 웹 크롤러가 그들이 방문한 곳을 관리하기 위해 사용하는 유용한 기법을 몇 가지 들어보면 다음과 같다.

* 트리와 해시 테이블
    * 방문한 URL을 추적하기 위해 검색 트리나 해시 테이블을 사용했을 수 있다. 
* 느슨한 존재 비트맵
    * 공간 사용을 최소화하기 위해, 몇몇 대규모 크롤러들은 존재 비트 배열(presence bit array)과 같은 느슨한 자료 구조를 사용한다. 각 URL은 해시 함수에 의해 고정된 크기의 숫자로 변환되고 배열 안에 대응하는 '존재 비트(presence bit)'를 갖는다. URL이 크롤링 되었을 때 해당하는 존재 비트가 만들어진다. 존재 비트가 이미 존재한다면 이미 크롤링 되었다고 간주한다.
* 체크포인트
    * 로봇 프로그램이 갑작스럽게 중단될 경우를 대비해, 방문한 URL의 목록이 디스크에 저장되었는지 확인한다.
* 파티셔닝
    * 웹의 성장으로, 한 대의 컴퓨터에서 하나의 로봇이 크롤링을 완수하는 것은 불가능해졌다. 몇몇 대규모 웹 로봇은, 각각이 분리된 한 대의 컴퓨터인 로봇들이 동시에 일하고 있는 '농장(farm)'을 이용한다. 각 로봇엔 URL들의 특정 '한 부분'이 할당되어 그에 대한 책임을 진다. 로봇들은 서로 도와 웹을 크롤링한다. 개별 로봇들은 URL들을 이리저리 넘겨주거나, 오동족하는 동료를 도와주거나, 활동을 조정하기 위해 커뮤니케이션을 할 필요가 있다.

### 별칭(alias)과 로봇 순환

URL이 별칭을 가질 수 있으므로 페이지의 방문 여부를 구분하기 쉽지 않을 수 있다. 한 URL이 또 다른 URL에 대한 별칭이라면 같은 리소스를 가리키고 있다.

### URL 정규화하기

웹 로봇은 URL들을 표준 형식으로 정규화함으로써 다른 URL과 같은 리소스를 가리키고 있는 경우를 미리 제거하려 시도한다.

1. 포트 번호가 명시되지 않았다면 호스트명에 ':80'을 추가한다.
2. 모든 %xx 이스케이핑된 문자들을 대응되는 문자로 변환한다.
3. '#' 태그들을 제거한다.

### 파일 시스템 링크 순환

심벌릭 링크 순환은 서버 관리자가 실수로 만들게 되는 것이 보통이지만, 때로는 '사악한 웹 마스터'가 로봇을 함정에 빠뜨리기 위해 악의적으로 만들기도 한다.

### 루프와 중복 피하기

잘 설계된 로봇은 순환을 피하기 위해 휴리스틱의 집합을 필요로 한다.

자율적인 크롤러는 문제를 피하는데 도움을 주지만 동시에 유효한 콘텐츠를 걸러버리게 되는 일도 일어날 수 있어 손실을 유발할 수 있다.

웹 로봇이 올바르게 동작하기 위해 사용하는 기법들은 다음과 같다.

* URL 정규화
* 너비 우선 크롤링: 순환의 영향을 최소화할 수 있다.
* 스로틀링: 일정 시간 동안 가져올 수 있는 페이지 숫자를 제한한다.
* URL 크기 제한: 일정 길이를 넘는 URL의 크롤링은 거부할 수 있다.
* URL/사이트 블랙리스트: 순환을 만들어 내거나 함정인 것으로 알려진 사이트와 URL 목록을 만들어 관리하고 블랙리스트에 추가한다.
* 패턴 발견: 순환 및 그와 비슷한 오설정들은 일정 패턴을 따르는 경향이 있다.
* 콘텐츠 지문(fingerprint): 페이지의 콘텐츠에서 몇 바이트를 얻어내어 체크섬을 계산한다. 이전에 보았던 체크섬을 가진 페이지를 가져온다면, 그 페이지의 링크는 크롤링하지 않는다.
* 사람의 모니터링

## 로봇의 HTTP

많은 로봇이 콘텐츠 요청을 위해 필요한 HTTP를 최소한으로만 구현하려고 한다. 결과적으로 많은 로봇이 요구사항이 적은 HTTP/1.0 요청을 보낸다. 

### 요청 헤더 식별하기

로봇의 능력, 신원, 출신을 알려주는 기본적인 몇 가지 헤더를 사이트에게 보내주는 것이 좋다. 잘못된 크롤러의 소유자를 찾아낼 때와 서버에게 로봇이 어떤 종류의 콘텐츠를 다룰 수 있는지에 대한 약간의 정보를 주려 할 때 모두 유용한 정보다.

로봇 개발자들이 구현을 하도록 권장되는 기본적인 신원 식별 헤더들은 다음과 같다.

* User-Agent: 서버에게 요청을 만든 로봇의 이름을 말해준다.
* From: 로봇의 사용자/관리자의 이메일 주소를 제공한다.
* Accept: 서버에게 어떤 미디어 타입을 보내도 되는지 말해준다. 로봇이 관심 있는 유형의 콘텐츠만 받게 될 것임을 확신하도록 해준다.
* Referer: 현재의 요청 URL을 포함한 문서의 URL을 제공한다.

### 가상 호스팅

로봇 구현자들은 Host 헤더를 지원할 필요가 있다. 가상 호스팅이 널리 퍼져 있는 현실에서 요청에 Host 헤더를 포함하지 않으면 로봇이 어떤 URL에 대해 잘못된 콘텐츠를 찾게 만든다.

### 조건부 요청

변경되었을 경우에만 콘텐츠를 가져오도록 하는 것은 로봇이 검색하는 콘텐츠의 양을 최소화함으로써 상당히 의미 있는 일이다.

시간이나 엔터티 태그를 비교함으로써 받아간 마지막 버전 이후에 업데이트된 것이 있는지 알아보는 조건부 HTTP 요청을 구현할 수 있다.

### 응답 다루기

HTTP의 특정 몇몇 기능을 사용하는 로봇들이나, 웹 탐색이나 서버와의 상호작용을 더 잘 해보려고 하는 로봇들은 여러 종류의 HTTP 응답을 다룰 줄 알 필요가 있다.

* 상태 코드
    * 로봇은 HTTP 상태 코드를 이해해야 한다. 
* 엔터티
    * HTTP 헤더에 임베딩된 정보를 따라 로봇들은 엔터티 자체의 정보를 찾을 수 있다.
    * 메타 http-equiv 태그와 같은 메타 HTML 태그는 리소스에 대해 콘텐츠 저자가 포함시킨 정보다. 

### User-Agent 타겟팅

웹 관리자들은 많은 로봇이 그들의 사이트를 방문하게 될 것임을 명심하고, 그 로봇들로부터의 요청을 예상해야 한다. 사이트 관리자들은 로봇의 요청을 다루기 위한 전략을 세워야 한다. 예를 들어, 특정 브라우저의 기능이 지원되는 것을 전제하여 콘텐츠를 개발하는 대신, 풍부한 기능을 갖추지 못한 브라우저나 로봇 등 다양한 클라이언트에 잘 대응하는 유연한 페이지를 개발할 수 있을 것이다. 사이트 관리자들은 최소한 로봇이 그들의 사이트에 방문했다가 콘텐츠를 얻을 수 없어 당황하는 일이 없도록 대비해야 한다.

## 부적절하게 동작하는 로봇들

* 폭주하는 로봇: 로봇이 논리적인 에러를 갖고 있거나 순환에 빠졌다면 웹 서버에 극심한 부하를 안겨줄 수 있으며, 서버 과부하를 유발하여 서비스를 못하게 만드는 일이 발생할 수 있다.
* 오래된 URL: 로봇들은 존재하지 않는 URL에 대한 요청을 많이 보낼 수 있다.
* 길고 잘못된 URL: 순환이나 프로그래밍상의 오류로 웹 사이트에게 크고 의미 없는 URL을 요청할 수 있다.
* 호기심이 지나친 로봇
* 동적 게이트웨이 접근: 게이트웨이 애플리케이션의 콘텐츠에 대한 URL로 요청을 할 수도 있다. 이 경우 얻은 데이터는 특수 목적을 위한 것일 테고 처리 비용이 많이 들 것이다.

## 로봇 차단하기

1994년 로봇이 맞지 않는 장소에 들어오지 않도록 하고 웹 마스터에게 로봇의 동작을 잘 제어할 수 있는 메커니즘을 제공하는 단순하고 자발적인 기법이 제안되었다. "Robots Exclusion Standard"라는 이름의 표준이며, robots.txt 라고 불린다.

웹 서버는 서버의 문서 루트에 robots.txt 이름의 선택적인 파일을 제공할 수 있다. 이 파일은 어떤 로봇이 서버의 어떤 부분에 접근할 수 있는지에 대한 정보가 담겨있다. 로봇이 이 표준에 따른다면 웹 사이트의 어떤 리소스에 접근하기 전에 우선 그 사이트의 robots.txt를 요청할 것이다.

### 웹 사이트와 robots.txt 파일들

웹 사이트의 어떤 URL을 방문하기 전에, 그 웹 사이트에 rotobs.txt 파일이 존재한다면 로봇은 반드시 그 파일을 가져와서 처리해야 한다. 웹 마스터는 웹 사이트의 모든 콘텐츠에 대한 차단 규칙을 종합적으로 기술한 robots.txt 파일을 생성할 책임이 있다.

### robots.txt 파일 포맷

예)
```
# 이 robots.txt 파일은 Slurp과 Webcrawler가 우리 사이트의 공개된 영역을 크롤링하는 것을 허락한다. 그러나 다른 로봇은 안된다...

User-Agent: slurp
User-Agent: webcrawler
Disallow: /private

User-Agent: *
Disallow:
```

레코드는 규칙 줄들의 집합으로 되어 있으며 빈 줄이나 파일 끝(end-of-file) 문자로 끝난다. 레코드는 어떤 로봇이 이 레코드에 영향을 받는지 지정하는 하나 이상의 User-Agent 줄로 시작하며 뒤이어 이 로봇들이 접근할 수 있는 URL들을 말해주는 Allow줄과 Disallow 줄이 온다.

* User-Agent
    * 각 로봇의 레코드는 하나 이상의 User-Agent 줄로 시작한다.
    * 형식: `User-Agent:<robot-name>`
* Disallow, Allow
    * 특정 로봇에 대해 어떤 URL 경로가 명시적으로 금지되어 있고 명시적으로 허용되는지 기술한다.
    * 로봇은 반드시 요청하려고 하는 URL을 차단 레코드의 모든 Disallow와 Allow 규칙에 순서대로 맞춰 보아야 한다. 첫 번째로 맞은 것이 사용된다. 만약 어떤 것도 맞지 않으면, 그 URL은 허용된다.

## 검색엔진

웹 로봇을 가장 광범위하게 사용하는 것은 인터넷 검색엔진이다.

웹 크롤러들은 마치 먹이를 주듯 검색엔진에게 웹에 존재하는 문서들을 가져다주어서, 검색엔진이 어떤 문서에 어떤 단어들이 존재하는지에 대한 색인을 생성할 수 있게 한다. 

### 현대적인 검색엔진의 아키텍처

오늘날 검색엔진들은 그들이 갖고 있는 전 세계의 웹페이지들에 대해 '풀 텍스트 색인(full-text indexes)'이라고 하는 복잡한 로컬 데이터베이스를 생성한다. 

검색엔진 크롤러들은 웹페이지들을 수집하여 집으로 가져와서, 이 풀 텍스트 색인에 추가한다. 동시에, 검색엔진 사용자들은 핫봇이나 구글과 같은 웹 검색 게이트웨이를 통해 풀 텍스트 색인에 대한 질의를 보낸다. 크롤링을 한 번 하는데 걸리는 시간이 상당한 데 비해 웹페이지들은 매 순간 변화하기 때문에, 풀 텍스트 색인은 기껏 해봐야 웹의 특정 순간에 대한 스냅숏에 불과하다.

### 풀 텍스트 색인

풀 텍스트 색인은 단어 하나를 입력받아 그 단어를 포함하고 있는 문서를 즉각 알려줄 수 있는 데이터베이스다. 이 문서들은 색인이 생성된 후에는 검색할 필요가 없다.

풀 텍스트 색인은 각 단어를 포함한 문서들을 열거한다.
