# 웹 로봇

웹 로봇의 몇 가지 예
* 주식시장 서버에 매 분 HTTP GET 요청을 보내고, 여기서 얻은 데이터를 활용해 주가 추이 그래프를 생성하는 주식 그래프 로봇
* 월드 와이드 웹의 규모와 진화에 대한 통계 정보를 수집하는 웹 통계 조사 로봇. 웹을 떠돌면서 페이지의 개수를 세고, 각 페이지의 크기, 언어, 미디어 타입을 기록한다.
* 검색 데이터베이스를 만들기 위해 발견한 모든 문서를 수집하는 검색엔진 로봇
* 상품에 대한 가격 데이터베이스를 만들기 위해 온라인 쇼핑몰의 카탈로그에서 웹피이지를 수집하는 가격 비교 로봇

## 크롤러와 크롤링

웹 크롤러는 웹 페이지를 한 개 가져오고, 그 다음 그 페이지가 가리키는 모든 웹페이지를 가져오며 재ㅔ귀적으로 반복하는 방식으로 웹을 순회하는 로봇이다. 웹 링크를 재귀적으로 따라가는 로봇을 **크롤러** 또는 **스파이더**라고 부른다.

### 어디에서 시작하는가: '루트 집합'

크롤러가 방문을 시작하는 URL들의 초기 집합을 **루트 집합**(root set)이라고 부른다. 루트 집합을 고를 때, 모든 링크를 크롤링하면 결과적으로 관심 있는 웹페이지들의 대부분을 가져오게 될 수 있도록 충분히 다른 장소에서 URL들을 선택해야 한다.

일반적으로 좋은 루트 집합은 크고 인기 있는 웹 사이트, 새로 생성된 페이지들의 목록, 그리고 자주 링크되지 않는 잘 알려져 있지 않은 페이지들의 목록으로 구성되어 있다.

### 링크 추출과 상대 링크 정상화

크롤러는 웹을 돌아다니면서 꾸준히 HTML 문서를 검색한다. 크롤러는 검색한 각 페이지 안에 들어있는 URL 링크들을 파싱해서 크롤링할 페이지들의 목록에 추가해야 한다. 링크들을 추출하고 상대 링크를 절대 링크로 변환할 필요가 있다.

### 순환 피하기

로봇이 웹을 크롤링할 때, 루프나 순환에 빠지지 않도록 매우 조심해야 한다. 로봇들은 순환을 피하기 위해 반드시 그들이 어디를 방문했는지 알아야 한다. 순환은 로봇을 함정에 빠뜨려서 멈추게 하거나 진행을 느려지게 한다.

### 루프와 중복

* 순환은 크롤러를 루프에 빠뜨려서 꼼짝 못하게 만들 수 있다.
* 크롤러가 같은 페이지를 반복해서 가져오면 웹 서버의 부담이 된다. 실제 사용자도 사이트에 접근할 수 없도록 막아버리게 될 수도 있으며, 법적인 문제제기의 근거가 될 수도 있다.
* 루프 자체가 문제가 되지 않더라도, 크롤러는 많은 수의 중복된 페이지들을 가져오게 된다.

### 빵 부스러기의 흔적

방문한 곳을 지속적으로 추적하는 것은 쉽지 않다. 어떤 URL을 방문했는지 빠르게 판단하기 위해서는 복잡한 자료 구조를 사용할 필요가 있다. 이 자료 구조는 속도와 메모리 사용 면에서 효과적이어야 한다.

수억 개의 URL은 빠른 검색 구조를 요구하기 때문에 빠른 속도는 중요하다. 로봇은 어떤 URL이 방문했던 곳인지 빠르게 결정하기 위해 적어도 검색 트리나 해시 테이블을 필요로 할 것이다.

대규모 웹 크롤러가 그들이 방문한 곳을 관리하기 위해 사용하는 유용한 기법을 몇 가지 들어보면 다음과 같다.

* 트리와 해시 테이블
    * 방문한 URL을 추적하기 위해 검색 트리나 해시 테이블을 사용했을 수 있다. 
* 느슨한 존재 비트맵
    * 공간 사용을 최소화하기 위해, 몇몇 대규모 크롤러들은 존재 비트 배열(presence bit array)과 같은 느슨한 자료 구조를 사용한다. 각 URL은 해시 함수에 의해 고정된 크기의 숫자로 변환되고 배열 안에 대응하는 '존재 비트(presence bit)'를 갖는다. URL이 크롤링 되었을 때 해당하는 존재 비트가 만들어진다. 존재 비트가 이미 존재한다면 이미 크롤링 되었다고 간주한다.
* 체크포인트
    * 로봇 프로그램이 갑작스럽게 중단될 경우를 대비해, 방문한 URL의 목록이 디스크에 저장되었는지 확인한다.
* 파티셔닝
    * 웹의 성장으로, 한 대의 컴퓨터에서 하나의 로봇이 크롤링을 완수하는 것은 불가능해졌다. 몇몇 대규모 웹 로봇은, 각각이 분리된 한 대의 컴퓨터인 로봇들이 동시에 일하고 있는 '농장(farm)'을 이용한다. 각 로봇엔 URL들의 특정 '한 부분'이 할당되어 그에 대한 책임을 진다. 로봇들은 서로 도와 웹을 크롤링한다. 개별 로봇들은 URL들을 이리저리 넘겨주거나, 오동족하는 동료를 도와주거나, 활동을 조정하기 위해 커뮤니케이션을 할 필요가 있다.

### 별칭(alias)과 로봇 순환

URL이 별칭을 가질 수 있으므로 페이지의 방문 여부를 구분하기 쉽지 않을 수 있다. 한 URL이 또 다른 URL에 대한 별칭이라면 같은 리소스를 가리키고 있다.

### URL 정규화하기

웹 로봇은 URL들을 표준 형식으로 정규화함으로써 다른 URL과 같은 리소스를 가리키고 있는 경우를 미리 제거하려 시도한다.

1. 포트 번호가 명시되지 않았다면 호스트명에 ':80'을 추가한다.
2. 모든 %xx 이스케이핑된 문자들을 대응되는 문자로 변환한다.
3. '#' 태그들을 제거한다.

### 파일 시스템 링크 순환

심벌릭 링크 순환은 서버 관리자가 실수로 만들게 되는 것이 보통이지만, 때로는 '사악한 웹 마스터'가 로봇을 함정에 빠뜨리기 위해 악의적으로 만들기도 한다.

### 루프와 중복 피하기

잘 설계된 로봇은 순환을 피하기 위해 휴리스틱의 집합을 필요로 한다.

자율적인 크롤러는 문제를 피하는데 도움을 주지만 동시에 유효한 콘텐츠를 걸러버리게 되는 일도 일어날 수 있어 손실을 유발할 수 있다.

웹 로봇이 올바르게 동작하기 위해 사용하는 기법들은 다음과 같다.

* URL 정규화
* 너비 우선 크롤링: 순환의 영향을 최소화할 수 있다.
* 스로틀링: 일정 시간 동안 가져올 수 있는 페이지 숫자를 제한한다.
* URL 크기 제한: 일정 길이를 넘는 URL의 크롤링은 거부할 수 있다.
* URL/사이트 블랙리스트: 순환을 만들어 내거나 함정인 것으로 알려진 사이트와 URL 목록을 만들어 관리하고 블랙리스트에 추가한다.
* 패턴 발견: 순환 및 그와 비슷한 오설정들은 일정 패턴을 따르는 경향이 있다.
* 콘텐츠 지문(fingerprint): 페이지의 콘텐츠에서 몇 바이트를 얻어내어 체크섬을 계산한다. 이전에 보았던 체크섬을 가진 페이지를 가져온다면, 그 페이지의 링크는 크롤링하지 않는다.
* 사람의 모니터링

## 로봇의 HTTP

많은 로봇이 콘텐츠 요청을 위해 필요한 HTTP를 최소한으로만 구현하려고 한다. 결과적으로 많은 로봇이 요구사항이 적은 HTTP/1.0 요청을 보낸다. 

### 요청 헤더 식별하기

로봇의 능력, 신원, 출신을 알려주는 기본적인 몇 가지 헤더를 사이트에게 보내주는 것이 좋다. 잘못된 크롤러의 소유자를 찾아낼 때와 서버에게 로봇이 어떤 종류의 콘텐츠를 다룰 수 있는지에 대한 약간의 정보를 주려 할 때 모두 유용한 정보다.

로봇 개발자들이 구현을 하도록 권장되는 기본적인 신원 식별 헤더들은 다음과 같다.

* User-Agent: 서버에게 요청을 만든 로봇의 이름을 말해준다.
* From: 로봇의 사용자/관리자의 이메일 주소를 제공한다.
* Accept: 서버에게 어떤 미디어 타입을 보내도 되는지 말해준다. 로봇이 관심 있는 유형의 콘텐츠만 받게 될 것임을 확신하도록 해준다.
* Referer: 현재의 요청 URL을 포함한 문서의 URL을 제공한다.

### 가상 호스팅


### 조건부 요청


### 응답 다루기


### User-Agent 타겟팅


## 부적절하게 동작하는 로봇들

